{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge #2: Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purpose of this challenge is to test candidates’ basic knowledge of Bayesian statistics and arithmetic skills. Using the data set provided, please write your own Bayesian algorithm to categorize map_publisher to channel for the given dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author : Rishi Sankineni "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the necessary Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wordninja \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the publisher channel mapping dataset using Pandas and read it into a DataFrame.\n",
    "pub_chan_map = pd.read_csv(\"publisher_channel_mapping.csv\",low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_publisher</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4music</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4seven</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5paidtelevision</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5paidtelevisionplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5select</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5spike</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5star</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5starplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5tv</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5tvplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          map_publisher         channel\n",
       "0                4music  paidtelevision\n",
       "1                4seven  paidtelevision\n",
       "2       5paidtelevision  paidtelevision\n",
       "3  5paidtelevisionplus1  paidtelevision\n",
       "4               5select  paidtelevision\n",
       "5                5spike  paidtelevision\n",
       "6                 5star  paidtelevision\n",
       "7            5starplus1  paidtelevision\n",
       "8                   5tv  paidtelevision\n",
       "9              5tvplus1  paidtelevision"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first 10 rows of our dataset to see how the data looks like.\n",
    "pub_chan_map.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paidtelevision    472\n",
       "paidradio          66\n",
       "paiddisplay         5\n",
       "paidsearch          3\n",
       "paidoutofhome       2\n",
       "paidvideo           1\n",
       "paidsocial          1\n",
       "Name: channel, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since channel is our class label, let's look at its distribution.\n",
    "pub_chan_map[\"channel\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Split text without spaces into list of words. We're mainly interested in splitting the text data in the map_publisher column, because there are no unique values which can be fed to our Bayesian(Naive Bayes) algorithm. For instance, let's say our input is \"5starplus1\" we'd like to split such text into list of words and get: ['5', 'star', 'plus', '1']. How do we do this efficiently? Well, we could probabilistically split concatenated words using NLP based on English Wikipedia unigram frequencies. We'd be using Wordninja, a Python package to perform this task.\n",
    "\n",
    "#### Wordninja GitHub repo: https://github.com/keredson/wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', 'star', 'plus', '1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how we split the nonspaced text using wordninja.\n",
    "import wordninja\n",
    "word_split = wordninja.split('5starplus1')\n",
    "word_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, the result for the above unspaced text is a \"list\" rather than a \"string\". So, we've to join those list of words to form a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 star plus 1\n"
     ]
    }
   ],
   "source": [
    "# this is how we convert a list of words into a sentence in Python.\n",
    "print (' '.join(word for word in word_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now let's apply the above logic to our entire dataset. Please note that we'd also be excluding the digits from our text, because I think the digits in the map_publisher field aren't that useful for our Bayesian model. So, our preprocessed text would only contain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## split the text in the \"map_publisher\" column and save it into a new column in our dataframe.\n",
    "pub_chan_map[\"new_map_publisher\"] = pub_chan_map['map_publisher'].apply(lambda x: wordninja.split(x))\n",
    "pub_chan_map[\"new_map_publisher\"] = pub_chan_map['new_map_publisher'].apply(lambda x: ' ' .join(word for word in x if not word.isdigit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the text in the \"channel\" column and save it into a new column in our dataframe.\n",
    "pub_chan_map[\"new_channel\"] = pub_chan_map[\"channel\"].apply(lambda x: wordninja.split(x))\n",
    "pub_chan_map[\"new_channel\"] = pub_chan_map[\"new_channel\"].apply(lambda x: ' '.join(word for word in x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is how our new dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_publisher</th>\n",
       "      <th>channel</th>\n",
       "      <th>new_map_publisher</th>\n",
       "      <th>new_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4music</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>music</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4seven</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>seven</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5paidtelevision</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>paid television</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5paidtelevisionplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>paid television plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5select</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>select</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5spike</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>spike</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5star</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>star</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5starplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>star plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5tv</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>tv</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5tvplus1</td>\n",
       "      <td>paidtelevision</td>\n",
       "      <td>tv plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          map_publisher         channel     new_map_publisher      new_channel\n",
       "0                4music  paidtelevision                 music  paid television\n",
       "1                4seven  paidtelevision                 seven  paid television\n",
       "2       5paidtelevision  paidtelevision       paid television  paid television\n",
       "3  5paidtelevisionplus1  paidtelevision  paid television plus  paid television\n",
       "4               5select  paidtelevision                select  paid television\n",
       "5                5spike  paidtelevision                 spike  paid television\n",
       "6                 5star  paidtelevision                  star  paid television\n",
       "7            5starplus1  paidtelevision             star plus  paid television\n",
       "8                   5tv  paidtelevision                    tv  paid television\n",
       "9              5tvplus1  paidtelevision               tv plus  paid television"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_chan_map.head(10)\n",
    "# the last 2 columns were generated from our preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's drop our old columns and only use the columns which were generated from our preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pub_chan_map.drop(['map_publisher','channel'], axis = 1, inplace=True) # drop the old columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_map_publisher</th>\n",
       "      <th>new_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seven</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paid television</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid television plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spike</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>star</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>star plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tv</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tv plus</td>\n",
       "      <td>paid television</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_map_publisher      new_channel\n",
       "0                 music  paid television\n",
       "1                 seven  paid television\n",
       "2       paid television  paid television\n",
       "3  paid television plus  paid television\n",
       "4                select  paid television\n",
       "5                 spike  paid television\n",
       "6                  star  paid television\n",
       "7             star plus  paid television\n",
       "8                    tv  paid television\n",
       "9               tv plus  paid television"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_chan_map.head(10) # this is how our new dataset looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Build a text categorization/classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the number of media channels per publisher is imbalanced. Advertisers are more biased towards Paid television and Paid radio.\n",
    "\n",
    "### Background: When we encounter such problems, we are bound to have difficulties solving them with standard algorithms. Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration. In the worst case, minority classes are treated as outliers and ignored. For some cases, such as  cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for example by undersampling or oversampling each class.\n",
    "\n",
    "### Takeaway: However, in our case of learning imbalanced data, the majority classes might be of our great interest. It is desirable to have a classifier that gives high prediction accuracy over the majority class, while maintaining reasonable accuracy for the minority classes. Therefore, we will leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAHCCAYAAAAgkophAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBrUkSQ0zqCVJatj/B8dzAQrEKXNnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1da917f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "pub_chan_map.groupby('new_channel').new_map_publisher.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data to vectors\n",
    "\n",
    "### Now we'll convert each publisher, represented as a string, into a vector that machine learning models can understand.\n",
    "\n",
    "### Now we need to transform the probability we want to calculate into something that can be calculated using word frequencies. For this, we will use some basic properties of probabilities, and Bayes’ Theorem. \n",
    "\n",
    "### Doing that requires essentially three steps, in the bag-of-words model:\n",
    "\n",
    "##### a) counting how many times does a word occur in each publisher (term frequency)\n",
    "##### b) weighting the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "##### c) normalizing the vectors to unit length, to abstract from the original text length (L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 54)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We will use sklearn.feature_extraction.text.TfidfVectorizer to calculate a tf-idf vector for each of our publishers:\n",
    "## Hyperparameters: sublinear_df, min_df, norm, encoding, ngram_range.\n",
    "# sublinear_df is set to True to use a logarithmic form for frequency.\n",
    "# min_df is the minimum numbers of documents a word must be present in to be kept.\n",
    "# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1.\n",
    "# ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(pub_chan_map.new_map_publisher).toarray()\n",
    "labels = pub_chan_map.new_channel\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, each of 550 media publishers is represented by 54 features, representing the tf-idf score for different unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways:\n",
    "\n",
    "### To train our bayesian classifier, we first transformed the “publisher_map” into a vector of numbers. We explored vector representations such as TF-IDF weighted vectors.\n",
    "\n",
    "### After having this vector representations of the text we can train our bayesian classifier to learn unseen “publishers” and predict the “media channel” on which they fall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all the above data transformation, now that we have all the features and labels, it is time to train our bayesian classifier. We will be using the Naive Bayes Classifier. The one most suitable for word counts is the multinomial variant of Naive Bayes.\n",
    "\n",
    "### Multinomial Naive Bayes: \"Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 75-25 train v/s test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(pub_chan_map['new_map_publisher'], pub_chan_map['new_channel'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, as requested, the test size is 25% of the entire dataset (138 publishers out of total 550), and the training is the rest (412 out of 550)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 138 550\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train), len(X_test), len(y_train) + len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross-validation: \n",
    "\n",
    "### \"A common practice is to partition the training set again, into smaller subsets; for example, 10 equally sized subsets. Then we train the model on nine parts, and compute accuracy on the last part (called \"validation set\"). Repeated ten time times (taking different part for evaluation each time), we get a sense of model \"stability\". If the model gives wildly different scores for different subsets, it's a sign something is wrong (bad data, or bad model variance). Go back, analyze errors, re-check input data for garbage, re-check data cleaning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    MultinomialNB(),\n",
    "]\n",
    "CV = 10 # 10-fold\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy: 96%. The scores are already high here, but how could we improve this model in the future?\n",
    "\n",
    "1) Get more training data with a balanced class distribution to overcome low model complexity. Our dataset has a pretty high class imbalance ratio, so \n",
    "the model may not be able to generalize well to new publishers from a media channel with a lower class distribution.\n",
    "\n",
    "2) Try different text preprocessing techniques like lemmatization, stemming, stopwords removal to generate more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_name\n",
       "MultinomialNB    0.961134\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 96% accuracy?? Did the model overfit? I'd say it's very unlikely our model would've overfit because Naive-Bayes is an example of high bias - low variance classifier ( aka simple and stable, not prone to overfitting) and we've also performed 10-fold cross-validation to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Being Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, let's see if our model is able to map a new(or unseen) publisher to a media channel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So here comes the Naive part: we assume that every word in a sentence is independent of the other ones. This means that we’re no longer looking at entire sentences, but rather at individual words. So for our purposes, “los angeles city network” is the same as “city los angeles network” and “network angeles city los”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paid radio']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(count_vect.transform([\"los angeles city network\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paid radio']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(count_vect.transform([\"city los angeles network\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we print out the classification report for each class: \n",
    "\n",
    "Only paid television has an f1, precision, recall scores because of our class distribution being so skewed towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    paid display       0.00      0.00      0.00         1\n",
      "paid out of home       0.00      0.00      0.00         1\n",
      "      paid radio       0.00      0.00      0.00        15\n",
      " paid television       0.88      1.00      0.93       121\n",
      "\n",
      "       micro avg       0.88      0.88      0.88       138\n",
      "       macro avg       0.22      0.25      0.23       138\n",
      "    weighted avg       0.77      0.88      0.82       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = clf.predict(count_vect.transform(y_test))\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
